\documentclass[a4wide, 11pt]{article}
\usepackage{a4, fullpage}
\setlength{\parskip}{0.4cm}
\setlength{\parindent}{0cm}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{float}
\usepackage[margin=1cm]{caption}
\usepackage[hmargin=1.7cm,vmargin=2.0cm]{geometry}

\begin{document}

\title{MAlice Report}

\author{Thomas Rooney and Alex Rozanski}

\date{\today}

\maketitle

\section {Introduction}

This project has been an introduction to compiler writing, through from the very start to the end. We have built, what we believe, to be a good example of a simple compiler, and a useful reference for the libraries that we have used should we or other look over it in attempting to do something similar. In this report we briefly outline some of the design decisions we have faced as well as try to critically evaluate our compiler and extension.

\section{Analysis of our Compiler}

\subsection{Exceptions}

One of the parts of our compiler that we feel is most useful is the syntactic and semantic error annotations. We were heavily inspired by clang's error messages in this regard, and display an ASCII `$\wedge$' character to `point' to the locations of such syntactic errors as missing tokens, and the tilde character to `underline' larger expressions, such as operands to a mathematical operator. This can be seen in the following example:

\begin{verbatim}
Semantic error (Line 6): Cannot match type `number' with expected types `boolean' for
operand `10' over operator &&.
    x became 10 && 20.
             ~~ ^^  
\end{verbatim}

\subsection{Extensibility}

\begin{itemize}
\item
ANTLR Grammar: The parser and lexer are built using ANTLR, which means that one can modify the AST with only a change to the BNF. This makes modifying/extending the language or changing the internal AST produced simple.
\item
Visitor Pattern: The way that we validate and produce code is via a programmatic construct that is a well known design pattern, and thus easily understandable by other software engineers. We seperate out the different nodes of our AST into methods meaning that changes made to the code can be localised to these particular leaves of the AST as opposed to having the chance of causing undefined behaviour. 
\item
Cross Compilable: Both Alex and Me work on Apple and Windows systems respectively, and our main target system was Linux. This caused a considerable number of issues at the beginning, especially when including LLVM libraries. As such, our code contains many preprocessor directives to ensure that equivilent libraries are included for each system, as well as preprocessor macros being set and unset dependent on the system and required environment for the LLVM environment.
\end{itemize}

\subsection{Criticisms}

At the start of milestone 3 we began off integrating code generation into the same visitor functions as those used by validation. The advantage of this was that the code was generated in the same pass as validation. However it quickly became apparent that this was causing some problems. For example, the LLVM libraries functions were treated as a ``black box'' and could cause exceptions that we couldn't produce useful error messages for in cases of incorrect ANTLR AST. To fix this, we decided to ensure that the validation pass was completely seperate to the Code generation pass, ensuring that error messages were as complete as possible.

This approach felt sub-optimal due to the fact that we were constructing internal compiler context information, such as scope and variable types, twice. To attune for this fact, we seperated code into a set of utility functions, and polymorphism in internal entities, shared such that code was not duplicated.

We also encountered a few problems due to our choice of language. We used RTTI (Real Time Type Information) in our compiler for downcasting from our symbol table. This was implemented via \texttt{dynamic\_cast}, which looking online, is a slow operation. It also lowers rigidity when we check the type as the order in which a variable is dynamically casted matters when you have deep class hierarchy.

\subsection{Benchmarks}

In building our compiler we also constructed an autotester. As part of this autotester, we construct benchmarks on the average time to process a file vs the average time it takes the reference compiler to process a file. This can be seen in `autotest.h' - where it shows that we, on average, are over ten times faster than the reference compiler for all of the given examples.
\begin{verbatim}
Average Time of Reference Compiler / Average Time of Our Compiler: 10.81822
\end{verbatim}

\subsection{Our Extension - DWARF Debugging Information}

For our extension, we took the principle that a language isn't complete until it can be effectively debugged. Rather than go for a full route of building up a debugging infrastructure however, we decided to build in metadata such that compiled Alice files can be debugged using existing debuggers which support the DWARF debugging format. This includes both gdb and lldb.

The end result of this extension is that a compiled alice file can be debugged in gdb. Breakpoints can be set in the .alice code, variables can be printed and code can be stepped through line by line. To include debugging information, pass the \texttt{-g} flag to the compile executable.

\begin{verbatim}
(gdb) break 5
Breakpoint 1 at 0x40056c: file ../malice_examples/valid/fibonacciIterative.alice, line 5.
(gdb) run
Starting program: /vol/bitbucket/tr111/malice_examples/valid/fibonacciIterative 
Which term in the Fibonacci sequence shall I compute?10

Breakpoint 1, fibonacci ()
    at ../malice_examples/valid/fibonacciIterative.alice:5
5     i became 0.
....
(gdb) next
8         temp was a number and temp became fib0.
(gdb) next
9         fib0 became fib1.
(gdb) print fib0
$2 = 0
(gdb) print fib1
$3 = 1
(gdb) next
10        fib1 became temp + fib1.
(gdb) print fib0
$4 = 1
\end{verbatim}

\section{Design Choices}

We chose to use C++ as it would allow us to build a compiler that runs very fast, whilst having useful features such as object orientation and the powerful STL. However, choosing a low level-language for its speed benefits also meant we had to handle and debug errors on a lower level, such as segmentation faults. This sometimes slowed down our productivity and development.

When we embarked on this project we both had the view that what we produced had to be as close as possible to what we would do, should we have to perform a similar task commercially but of greater complexity. As such, we built up a product around the idea of extensibility and ease of development. We tried to maximise the use of tools and have a simple, easily understandable design.

For our initial lexing and parsing stages, we decided to use ANTLR. It is a well-established tool which has the advantage of a GUI for editing and debugging which makes the development process much smoother - being able to see the parsing stages and AST generation step-by-step was incredibly useful when things went wrong. ANTLR can generate code for multiple languages, and although we had decided to use C++ for our compiler, we used the backend which generated a C lexer and parser from our BNF. We made this decision based on some preliminary research which suggested that the backend producing C++ code was less complete. 

Disadvantages of using ANTLR include the fact that sometimes the lexer errors produced were not very informative, and were missing vital information such as line or column numbers, and we had little or no control over this. The ANTLR-produced AST was also very rigid, which made it very difficult to modify the tree directly through optimisation, which was our initial plan.

The second major decision that we made was in the use of the LLVM C++ backend for the code generation stage, where we use LLVM functions to generate a tree of constructs for the different statements in LLVM IR (instructions such as \texttt{load} and \texttt{store} and function and variable declarations). We made this decision based on the fact that it seemed very well-documented at the time, and this would make adding extra features easier; once we had LLVM classes set up correctly, it is simple to activate the other LLVM libraries that add extra features into production compilers.

For example, in our final steps where we move code from LLVM classes into LLVM IR, we can retarget our compiler at any particular architecture that LLVM has support for. In fact, when we generate code, we pull the targeting information from the host system directly, and generate compilable code for that particular processor. If our compiler was run on, for example, an ARM processor, it wouldn't generate x86 assembler but ARM assembler - a massive advantage over focusing on a particular architecture.

One of the major disadvantages of this approach was the lack of reference material in using the C++ backend. At times it seemed like everything was going well, but in fact bits of the backend hadn't been implemented yet. From our experience using the libraries directly, it seems like while the majority of it is very generic, there are sections of the code base that are very specific to those few projects that use LLVM directly (like clang). It seems like it was only partially implemented in places such that -- for example -- clang generates code correctly, but the moment you call a function differently to the code in clang, output code is incorrectly generated.

An example of this is in the debug information for functions. Clang uses \texttt{llvm::DIType()} to denote an unknown or \texttt{void} return type for the function, and this works correctly, should the linkage and function names be the same. However, The moment they are different (debug name `hatta' and linkage name `main'), \texttt{!loc 0 0 0} is placed as a header of the function (in x86 Asm). This tries to set the debugger's current line number information to \texttt{[Null, Line 0, Col 0]}, an error that blocks compilation. We solved this problem by generating a \texttt{llvm::DISubroutineType} of \texttt{void}, with size/alignment/encoding \texttt{0} instead of the default \texttt{llvm::DIType()}. 

\section{Beyond the Specification}

The purpose of this project is to learn as much as possible about the construction and internals of Compilers, and as such the more interesting directions for extending the project should be under via the overtone of educating oneself. We have covered one of these directions, looking at providing the information debuggers require give meaningful debugging information. 

However there are other more interesting directions that we could have gone in were this project to be extended. These include
\begin{itemize}
\item Preprocessor
\item Interpreter
\item Language Extensions - object-orientation and dynamic memory allocation.
\item Disassembly
\end{itemize}
\end{document}
